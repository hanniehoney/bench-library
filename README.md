# llm-benchmark
A list of comprehensive LLM evaluation frameworks. Contributions welcome!

Benchmarks With 100+ Tasks
| Benchmark | Release Date | Repository | Paper/Blog | Task Number | Aspect | Licence |
| --- | --- | --- | --- | --- | --- | --- |
| HELM | --- | --- | --- | --- | --- | --- |
| BIG-bench | --- | https://github.com/google/BIG-bench | [Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models](https://openreview.net/pdf?id=uyTL5Bvosj) | --- | --- | --- |
| BigBIO | --- | https://github.com/bigscience-workshop/biomedical | [BigBio: A Framework for Data-Centric Biomedical Natural Language Processing](https://proceedings.neurips.cc/paper_files/paper/2022/file/a583d2197eafc4afdd41f5b8765555c5-Paper-Datasets_and_Benchmarks.pdf) | --- | --- | --- |
| BigSicence | --- | https://github.com/bigscience-workshop/evaluation | --- | --- | --- | --- |
| Language Model Evaluation Harness | --- | https://github.com/EleutherAI/lm-evaluation-harness | [Evaluating Large Language Models (LLMs) with Eleuther AI](https://wandb.ai/wandb_gen/llm-evaluation/reports/Evaluating-Large-Language-Models-LLMs-with-Eleuther-AI--VmlldzoyOTI0MDQ3) [Evaluating LLMs](https://www.eleuther.ai/projects/large-language-model-evaluation) | --- | --- | --- |
| Code Generation LM Evaluation Harness | --- | https://github.com/bigcode-project/bigcode-evaluation-harness | --- | --- | --- | --- |

Benchmarks With 5+ Tasks
| Benchmark | Release Date | Repository | Paper/Blog | Task Number | Aspect | Licence |
| --- | --- | --- | --- | --- | --- | --- |
| GLUE | --- | https://github.com/nyu-mll/jiant | --- | 11 | --- | --- |
| SuperGLUE | --- | https://github.com/nyu-mll/jiant | --- | 10 | --- | --- |
| CLUE | --- | https://github.com/CLUEbenchmark/CLUE | --- | 9 | --- | --- |
| CodeXGLUE | --- | https://github.com/microsoft/CodeXGLUE | --- | 10 | --- | --- |
