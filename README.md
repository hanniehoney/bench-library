# Benchmark Library
<img src="image.png" alt="Bench-Library" width="512"/>

This collection inspired by the amazing work of [llm_benchmarks](https://github.com/leobeeson/llm_benchmarks) and [llm-benchmark](https://github.com/terryyz/llm-benchmark). Weâ€™re all about collecting a diverse range of benchmarks that bring new perspectives to the table. Contributions are always welcome!

## Benchmarks

| Capability | Benchmark | Description | Resource |
| --- | --- | --- | --- |
| Knowledge and Language Understanding |
|  | Massive Multitask Language Understanding (MMLU) | Measures general knowledge across 57 different subjects, ranging from STEM to social sciences. | [GitHub](https://github.com/hendrycks/test) 
| |General Language Understanding Evaluation (GLUE) |A collection of various language tasks from multiple datasets, designed to measure overall language understanding. | [Website](https://gluebenchmark.com/) 
|  | SuperGLUE | An advanced version of the GLUE benchmark, comprising more challenging and diverse language tasks. | [Dataset](https://huggingface.co/datasets/super_glue) 
| Reasoning |
|  | AI2 Reasoning Challenge (ARC) | Tests LLMs on grade-school science questions, requiring both deep general knowledge and reasoning abilities. | [Dataset](https://huggingface.co/datasets/ai2_arc) 
|  | HellaSwag | Tests natural language inference by requiring LLMs to complete passages in a way that requires understanding intricate details. | [GitHub](https://github.com/rowanz/hellaswag/tree/master/data) 
|  | Discrete Reasoning Over Paragraphs (DROP) | An adversarially-created reading comprehension benchmark requiring models to navigate through references and execute operations like addition or sorting. | [Dataset](https://huggingface.co/datasets/drop) 
| Coding | 
|  | HumanEval | Contains programming challenges for evaluating LLMs' ability to write functional code based on instructions. | [GitHub](https://github.com/openai/human-eval)
|  | CodeXGLUE | Evaluates LLMs' ability to understand and work with code across various tasks like code completion and translation.| [GitHub](https://github.com/microsoft/CodeXGLUE) 
| Multi Turn Open Ended Conversations |
|  | MT-bench | Tailored for evaluating the proficiency of chat assistants in sustaining multi-turn conversations. | [Dataset](https://huggingface.co/datasets/lmsys/mt_bench_human_judgments) 
|  | Question Answering in Context (QuAC) | Features 14,000 dialogues with 100,000 question-answer pairs, simulating student-teacher interactions. | [Website](https://quac.ai/) 
| Math |
|  | Grade School Math 8K(GSM8K)| It consists of 8,000 grade school math problems that require multi-step reasoning to solve.| [Dataset](https://huggingface.co/datasets/openai/gsm8k) 
|  | MATH |The MATH dataset consists of 12,500 problems derived from high school math competitions. | [Dataset](https://paperswithcode.com/sota/math-word-problem-solving-on-math) 
<!-- |  | --- | --- | []() 
|  | --- | --- | []() 
|  | --- | --- | []()  -->
