# llm-benchmark
A list of comprehensive LLM evaluation frameworks. Contributions welcome!

| Benchmark | Release Date | Repository | Paper/Blog | Task Number (10+) | Aspect | Licence |
| --- | --- | --- | --- | --- | --- | --- |
| HELM | --- | --- | --- | --- | --- | --- |
| BigBIO | --- | https://github.com/bigscience-workshop/biomedical | [BigBio: A Framework for Data-Centric Biomedical Natural Language Processing](https://proceedings.neurips.cc/paper_files/paper/2022/file/a583d2197eafc4afdd41f5b8765555c5-Paper-Datasets_and_Benchmarks.pdf) | --- | --- | --- |
| Big | --- | --- | --- | --- | --- | --- |
| Language Model Evaluation Harness | --- | --- | [Evaluating Large Language Models (LLMs) with Eleuther AI](https://wandb.ai/wandb_gen/llm-evaluation/reports/Evaluating-Large-Language-Models-LLMs-with-Eleuther-AI--VmlldzoyOTI0MDQ3) | --- | --- | --- |
| Code Generation LM Evaluation Harness | --- | https://github.com/bigcode-project/bigcode-evaluation-harness | --- | --- | --- | --- |
| CodeXGLUE | --- | https://github.com/microsoft/CodeXGLUE | --- | 10 | --- | --- |
